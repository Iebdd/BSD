\documentclass{article}

\usepackage{geometry}
\usepackage{makecell}
\usepackage{array}
\usepackage{multicol}
\usepackage{setspace}
\usepackage{changepage}
\usepackage{booktabs}
\usepackage{cancel}
\usepackage{graphicx}
\usepackage[explicit]{titlesec}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{cprotect}
\usepackage{float}
\newcolumntype{?}{!{\vrule width 1pt}}
\newcommand{\paragraphlb}[1]{\paragraph{#1}\mbox{}\\}
\newcommand{\subparagraphlb}[1]{\subparagraph{#1}\mbox{}\\}
\renewcommand{\contentsname}{Inhaltsverzeichnis:}
\renewcommand\theadalign{tl}
\def\ojoin{\setbox0=\hbox{$\bowtie$}%
  \rule[-.02ex]{.25em}{.4pt}\llap{\rule[\ht0]{.25em}{.4pt}}}
\def\lojoin{\mathbin{\ojoin\mkern-5.8mu\bowtie}}
\def\rojoin{\mathbin{\bowtie\mkern-5.8mu\ojoin}}
\def\fojoin{\mathbin{\ojoin\mkern-5.8mu\bowtie\mkern-5.8mu\ojoin}}
\newcommand{\ajoin}{\ensuremath{\triangleright}}
\newcommand{\sjoin}{\ensuremath{\ltimes}}
\setstretch{1.10}
\setlength{\parindent}{0pt}
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

\titleformat{\section}
  {\normalfont\Large\bfseries}{\thesection}{1em}{\hyperlink{sec-\thesection}{#1}
\addtocontents{toc}{\protect\hypertarget{sec-\thesection}{}}}
\titleformat{name=\section,numberless}
  {\normalfont\Large\bfseries}{}{0pt}{#1}

\titleformat{\subsection}
  {\normalfont\large\bfseries}{\thesubsection}{1em}{\hyperlink{subsec-\thesubsection}{#1}
\addtocontents{toc}{\protect\hypertarget{subsec-\thesubsection}{}}}
\titleformat{name=\subsection,numberless}
  {\normalfont\large\bfseries}{\thesubsection}{0pt}{#1}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\geometry{top=12mm, left=1cm, right=2cm}
\title{\vspace{-3cm}Datenbankdesign}
\author{Andreas Hofer}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	\section{Kanonische Überdeckung}
	Eine kanonische Überdeckung $F_c$ ist eine minimale Menge von funktionalen Abhängigkeiten welche noch äquivalent zu der Menge F sind. Da im schlechtesten Fall die kanonische Menge die Menge selbst ist, hat jede Menge von funktionalen Abhängigkeiten eine kanonische Menge. Eine Menge kann auch mehrere kanonische Überdeckungen haben. \\
	Eine Menge ist eine kanonische Überdeckung wenn:
	\begin{enumerate}
		\item{$F_c^+=F^+$ -> Die beiden Mengen sind äquivalent}
		\item{$F_c$ hat keine weiteren Abhängigkeiten $X\ \to\ Y$ welche überflüssig sind.}
		\item{Alle linken Terme sidn einzigartig}
	\end{enumerate}
	\subsection{Vorgang}
	Man berechnet eine kanonische Überdeckung indem man folgende Schritte ausführt:
	\begin{enumerate}
		\item{Linksreduktion}
		\begin{itemize}
			\item{Die Anzahl der linken Terme wird reduziert}
		\end{itemize}
		\item{Rechtsreduktion}
		\begin{itemize}
			\item{Die Anzahl der rechten Terme wird reduziert}
		\end{itemize}
		\item{Entfernen von leeren Mengen}
		\item{Vereinigung}
		\begin{itemize}
			\item{Man wendet eventuell anfallende Armstrong-Axiome selbst an.}
		\end{itemize}
	\end{enumerate}
	Zu beachten ist, dass in diesem Prozess eine der möglichen kanonischen Überdeckungen gefunden wird.
	\subsubsection{Linksreduktion}
	Für jedes $X\ \to\ Y$ wird eine Linksreduktion durchgeführt:
	\begin{itemize}
		\item{Für alle Attribute A von X wird überprüft ob:}
		\begin{itemize}
			\item{$Y \subseteq H(F, X - A)$}
			\item{Ist Y äquivalent zur Hülle $X-A$}
		\end{itemize}
		\item{Wenn das gilt, dann wird X durch $X-A$ ersetzt -> $X-A\ \to\ Y$}
	\end{itemize}
	Ein paar Regeln zu Vereinfachung des Vorgangs ist, dass, wenn links jeweils nur ein Attribut steht, diese nie reduziert werden können.
	Wenn man
	\subsubsection{Rechtsreduktion}
	Für jedes $X\ \to\ Y$ wird eine Rechtsreduktion durchgeführt:
	\begin{itemize}
		\item{Für alle Attribute B wird überprüft ob:}
		\begin{itemize}
			\item{$B\in H(F-\{X\to Y\}\cup \{\})$}
		\end{itemize}
	\end{itemize}
	\subsubsection{Entfernung der leeren Mengen}
	Alle FDs der Form $X\ \to\ \null$ werden entfernt, welche eventuell durch die Rechtsreduktion entstanden sind.
	\subsubsection{Vereinigung}
	Neue FDs, welche entstanden sind, sollten vereinfacht werden. \\
	\verb|X->A, X->C, X->F wird zu X->ACF|
	\paragraphlb{Beispiel}

	\section{Datenbankzerlegung (Decomposition)}
	Da eine Datenbank sich (wenn sie aktiv verwendet wird) stets verändert, muss man diese regelmäßig verändern um weiterhin einer Normalform zu entsprechen.
	Bei der Zerlegung kann es speziell bei höheren Normalformen passieren, dass man eine Abhängigkeit nicht weiter abbilden kann. Also kann es unter Umständen passieren, dass eine funktionale Abhängigkeit aufgegeben werden muss. Was wiederum \textbf{niemals} passieren darf, ist, dass Daten verloren gehen. Alle Daten weiter zu besitzen ist das wichtigste Gut bei der Überführung einer Datenbank. \\
	Bei der Zerlegung wird eine Tabelle in seine Schemata übersetzt. Diese müssen am Ende vereint wiederum die Tabelle ergeben. \\
	Eine Zerlegung hat zwei Gütekriterien, die Verlustlosigkeit und die Abhängigkeitsbewahrung. \\
	\subsection{Verlustlosigkeit}
	Eine Zerlegung ist genau dann verlustlos, wenn ein Join aller Teile wiederum zur Tabelle führt: $\pi_{sch(R_1)}(R)\bowtie\pi_{sch(R_2)}(R)\bowtie ... \bowtie\pi_{sch(R_n)}(R)=R$ \\
	Da man hier einen natural join verwendet, wodurch es seinen Partner findet, sollte es automatisch wieder zur Tabelle vereint werden. Bei der Verlustlosigkeit muss beachtet werden, dass es stets nur um die Bewahrung der Information und nicht der Tupel geht. Wenn eine Tabelle zerlegt wird, werden Duplikate entfernt. \\
	Bei der Bestimmung der Verlustlosigkeit sollte man zuerst die gegebene Instanz untersuchen, die Tabelle aufteilen und danach überprüfen ob diese verlustfrei war. Wenn das gegeben ist kann man es allgemein überprüfen ob es für alle Instanzen gilt. \\
	Dabei wird mit der Hülle überprüft, ob

	\subsubsection{Enschränkungen}
	Einschränkungen bestimmen, für welche Relationen
	Das geschieht durch einen bestimmten Prozess, wobei für jede echte Teilemenge der gleiche Prozess durchlaufen wird:
	\begin{enumerate}
		\item{Triviale FDs werden entfernt (Wie zB. $X\to X$)}
		\item{Nur die Attribute beachten, welche vorkommen}
		\item{Falls Attribute vorhanden sind, werden diese hinzugefügt.}
	\end{enumerate}
	\paragraphlb{Beispiel}
	Gegeben ist das Schema $F_R$=\{$A\to D$, $D\to C$, $C\to D$\} mit der Zerlegung von R zu $R_1$[A, C] und $R_2$[C, D]. \\
	Zur Berechnung der Einschränkung sowie der Abhängigkeitsbewahrung muss zuerst die Hülle der Teile der Zerlegung berechnet werden: \\
	H(F, A)\\\{A\}=\{D, C\} \verb|Geschnitten mit A, C ergibt das A -> C|\\
	H(F, C)\\\{C\}=\{D\} \verb|Geschnitten mit A, C ergibt C->0 und mit C, D ergibt C->D|\\
	H(F, D)\\\{D\}=\{C\} \verb|Geschnitten mit C, D ergibt D->C|\\
	Aus diesem Grund sind die Einschränkungen von $R_1\ A\to C$ und von $R_2\ C\to D, D\to C$ \\
	Wenn man nun herausfinden will, ob die Abhängigkeiten bewahrt werden, muss man überprüfen ob $F^+_R=(F_{R_1}\cup F_{R_2})$ also ob $\{A\to D, D\to C, C\to D\}^+=\{A\to C, C\to D, D\to C\}^+$ \\
	Das erste Kriterium kann als gegeben angenommen werden, da sowohl $F_{R_1}$ als auch $F_{R_2}$ aus $F_R$ hergeleitet wurde. (Vorrausgesetzt man hat richtig gerechnet.) \\
	\section{Normalformen}
	Diese Zerlegungen sowie die Überprüfung ob Information verloren gegangen ist dient dazu eine Datenbank in eine bessere Form zu bringen. Eine dieser Güteklassen sind die Normalformen. Dabei existieren die erste bis zur fünften Normalformen (1NF, 2NF, 3NF, 4NF, 5NF) sowie die sogenannte Boyce-Codd Normalform (BCNF). In der Praxis finden jedoch meist nur die ersten drei, sowie die Boyce-Codd Normalform Anwendung. Für die vierte und fünfte ist der Rechenaufwand in Relation zu den Vorteilen unverhältnismäßig hoch. Aus diesem Grund werden diese auch in dieser Vorlesung nicht behandelt. Es sollte beachtet werden, dass Normalformen stets niedrigere Normalformen auch enthält. Also hat die zweite Normalform ebenfalls die Güte der ersten Normalform.
	\subsection{Normalisierung}
	Die Normalisierung ist der Prozess die Normalform einer Datenbank zu erhöhen. Dabei werden meist Attribute eines (schlechten) Schemas in kleinere (gute) Schemas aufgeteilt werden, welche den Normalformen genügen. Die Qualität einer Datenbank kann jedoch nur aufgrund verschiedener Tests bestimmt werden und besagt nicht, dass es auch ein gutes Schema hat. \\
	Eine Denormalisierung passiert hingegen, wenn mehrere Relationen einer Normalform als Relation gespeichert werden. Dieses Ergebnis hat höchstwahrscheinlich eine niedrigere Normalform als dessen Ursprung.
	\subsection{Erste Normalform / 1NF}
	Die erste Normalform ist die niederwertigste Normalform und verbietet zusammengesetzte oder mehrwertige Attribute. Strategien um die 1NF zu erreichen können sein:
	\begin{itemize}
		\item{Aus zusammengesetzten Attributen: Jeden Teil zum eigenen Attribut konvertieren}
		\item{Mehrwertige Attribute: Neues Tupel für jeden Wert des mehrwertigen Attributs}
		\item{Geschachtelte Relationen: Erzeuge ein neues Tupel für jedes Tupel der geschachtelten Relation}
	\end{itemize}
	\begin{tabular}{| l | l | l | l |}
		\toprule
		MName & MNum & MSVN & Standort \\ \midrule
		Martin & 1 & 12345678 & \{Graz, Linz, Salzburg\} \\ \hline
		
		\bottomrule
	\end{tabular}
	\subsection{Zweite Normalform / 2NF}
	Die zweite Normalform wird erreicht, wenn jedes nicht-Schlüsselattribut voll funktional abhängig von allen Kandidatenschlüsseln ist. In anderen Worten darf es keine Spalte geben, welcher nicht durch den Kandidatenschlüssel eindeutig identifizierbar ist. Die mathematische Definition ist: $K \to\, A \iff K \to\, A \land \forall X \subset K:X \not\to A$
	Strategien sind erneut:
	\begin{itemize}
		\item{Für jeden Teilschlüssel eine neue Relation mit seinen abhängigen Attributen generieren.}
	\end{itemize}
	Während jedoch keine direkten Abhängigkeiten erlaubt sind, können diese immer noch transitiv voneinander abhängig sein.
	\subsection{Dritte Normalform / 3NF}
	Die dritte Normalform wird erreicht, wenn jeder Superschlüssel X mindestens eine der Vorraussetzungen erfüllt: 
	\begin{enumerate}
		\item{X ist trivial}
		\item{X ist ein Superschlüssel von Y}
		\item{Es darf kein Attribut geben, welches in der Relation nicht vom Kandidatenschlüssel X abhängig ist.}
	\end{enumerate}

	\subsubsection{Zerlegung in 3NF}
	Man kann mittels des Synthesealgorithmus eine große Tabelle in kleinere Tabellen zerlegen. Diese Tabellen sind danach alle in der dritten Normalform. Gleichzeitig ist diese Zerlegung auch Verlustfrei und abhängigkeitsbewahrend. \\
	Zur Berechnung muss man:
	\begin{enumerate}
		\item{Die Kanonische Überdeckung berechnen}
		\item{Für jede Funktionale Abhängigkeit aus der kanonischen Überdeckung:}
		\begin{itemize}
			\item{Erstelle eine Relation welche die Abhängigkeit $X \to Y$ vereinigt.}
			\item{Diese Relation erhält die FDs}
		\end{itemize}
		\item{Falls keine der neuen Relationen einen Kandidatenschlüssel enthält, wähle einen K aus dem Schema.}
		\begin{itemize}
			\item{Erstelle dadurch eine Relation mit dem Schema K}
			\item{}
		\end{itemize}
	\end{enumerate}
	\subsubsection{Boyce-Codd-Normalform / BCNF}
	Die Boyce-Codd-Normalform (BCNF) und die 3NF haben relativ ähnliche Vorraussetzungen, jedoch schränkt die BCNF das Schema noch etwas mehr ein. Während bei der dritten Normalform jeder Kandidatenschlüssel eine der drei Vorraussetzungen erfüllen musste, ist die dritte in der BCNF nicht mehr zulässig. Dadurch können in der BCNF nicht mehr alle funktionalen Abhängigkeiten bewahrt werden, was bis zur 3NF möglich war.
	\paragraphlb{Dekompositionsalgorithmus}
	Der Dekompoisitionsalgorithmus dient zur Konvertierung einer Relation in die BCNF. Hierbei werden jedoch eventuell nicht alle Funktionalen Abhängigkeiten bewahrt. Die Schritte sind:
	\begin{enumerate}
		\item{}
	\end{enumerate}
	\subsection{Übung:}
	R[A, B, C, D, E, G] mit F = $\{A\to BD, AB\to E, B\to EG, C\to AB\}$

	Linksreduktion: Da nur AB mehr als ein Element besitzt, kann man dieses als einziges eventuell reduzieren.
	Wenn man das A bei $AB\to E$ wegstreichen will, muss man zuerst die Hülle berechnen um das zu bestätigen. Dazu muss man die Hülle bestimmen um nachzuprüfen, ob man trotzdem auf die gleiche Abhängigkeit kommt. \\
	$\cancel{A}B\to E$ \texttt{->} $H(F, B)$=\{E, G\} \texttt{-> Deshalb kann man A weglassen}.\\
	$A\cancel{B}\to E$ \texttt{->} $H(F, B)$=\{B. D, E, G\} \texttt{-> Deshalb kann man auch B weglassen. Aus diesem Grund kann man die gesamte Abhängigkeit weglassen}\\
	Rechtsreduktion: Hierbei kann man nicht sofort das Ergebnis sehen, aber ungefähr erkennen welche Teile reduziert werden können. $C\to AB$ kann eventuell reduziert werden. Hierbei muss man wieder die Hülle berechnen, wobei jedoch alle Attribute der Relation gegeben sein müssen. \\
	$C\to A\cancel{B}$ \texttt{->} H(F, C)=\{A, B, D, E, G\} \texttt{B kann reduziert werden.} \\
	Jetzt hat man die kanonische Überdeckung $F_C=\{A\to BD, B\to EG, C\to A\}$ und aus dieser kann man die neuen Relationen generieren.
	$R_A[A, B, D], F_A=\{A\to BD\}$ \\
	$R_B[B, E, G], F_B=\{B\to EG\}$ \\
	$R_C[C, A], F_C=\{C\to A\}$ \\
	\section{Normalform - Cheat Sheet}
	\begin{tabular}{| l | c |}
		\toprule
		Normalform & Vorraussetzung \\ \midrule
		1NF & Keine Zusammengesetzten/Mehrwertigen Attribute \\ \hline
		2NF & Alle Attribute durch Kandidatenschlüssel identifizierbar \\ \hline
		
		\bottomrule
	\end{tabular}
	\section{Speichermedien}
	Da sich Datenbanken stets mit der Speicherung von Daten befassen, ist die Art des Speichermediums stets eine relevante Überlegung. Dabei unterscheidet man zwischen verschiedenen Faktoren, unter anderem Kosten, Kapazität und Vefügbarkeit. \\
	Zusätzlich unterscheidet man zwischen den unterschiedlichen herarchischen Ordnungen von Speicher (Siehe Betriebssysteme Grundlagen). Dabei bildet der Speicher eine Pyramide welche einen Kompromiss aus Schreibgeschwindigkeit und Kapazität darstellt. \\
	Jedes Speichermedium benötigt außerdem noch einen Controller, welcher die Schnittstelle zwischen der physischen Platte und dem Computer darstellt (Siehe Betriebssysteme Grundlagen). Auf Festplatten
	Diese sind:
	\begin{itemize}
		\item{Cache}
		\begin{itemize}
			\item{Schnellster aber auch teuerster Speicher. Wird direkt von der CPU verwaltet}
		\end{itemize}
		\item{RAM (Hauptspeicher)}
		\begin{itemize}
			\item{Trotzdem noch schneller Zugriff (< 100ns) mit Datentransferraten von 25 GB/s. Dieser Speicher ist jedoch meist zu klein um eine gesamte Datenbank zu speichern.}
		\end{itemize}
		\item{Festplatte}
		\begin{itemize}
			\item{Der erste nicht-flüchtige Speicher (Verfällt nicht sobald kein Strom gegeben ist.)}
			\item{Bedeutend langsamer als RAM oder Cache (Im ms Bereich)}
			\item{Daten werden auf Magnetscheiben gespeichert.}
			\item{Kostet nur mehr 25€/TB (Statt etwa 4000€ für RAM)}
			\item{Im Schnitt verdoppelt sich die verfügbare Kapazität alle 2 bis 3 Jahre}
		\end{itemize}
		\item{Solid State Drive (SSD)}
		\begin{itemize}
			\item{Bedeutend schneller mit geringeren Zugriffszeiten als die HDD}
			\item{Schnellester persistenter Speicher}
			\item{Im Embedded Systems schon seit längerem verbreitet.}
			\item{Spezialform: EEPROM ((Electronically Erasable Programmable Read-Only Memory)) kann durch elektrische Impulse neu beschrieben werden.}
			\item{Hat verschiedene Zugriffsmuster:}
			\begin{itemize}
				\item{Random-access: Kann in beliebiger Reihenfolge gelesen werden.}
				\item{Block-based: Nicht in Bytes sondern in Blöcken (Von z.B. 4096 Bytes)}
			\end{itemize}
			\item{Optische Medien}
			\begin{itemize}
				\item{Daten werden von einer Platte via Laser gelesen und geschrieben.}
				\item{Hat verschiedene Typen. Von CDs (640MB) bis Blu-Ray (25GB) mit unterschiedlich schnellen Zugriffszeiten}
			\end{itemize}
		\end{itemize}
		\item{Magnetband}
		\begin{itemize}
			\item{Rein sequentieller Speicher. Dadurch sehr langsam.}
			\item{Kostet nur mehr 10€/TB}
		\end{itemize}
	\end{itemize}
	Die Forschung befasst sich momentan damit die Zugriffszeiten von persistenten Speichern zu reduzieren. Ein solches Produkt ist NVRAM (Non-Volatile Memory), welcher eine bedeutend geringere Latenz besitzt. Er ist zwar bedeutend günstiger als Ram, ist mit maximal 512GB an Größe jedoch bedeutend kleiner. Solche NVRAM-Speicher sind für Datenbanken sehr interessat, da sie viel schnellere Zugriffszeiten ermöglichen. \\
	\subsection{Festplatten}
	Aufgrund ihrer geringen Kosten und relativ schnellen Lesegeschwindigkeiten sind Festplatten das wichtigste Medium für Datenbanken.
	\subsubsection{Zugriffszeiten}
	Zur Berechnung der Zugriffszeit eines Speichers muss man einige Schritte beachten: 
	\begin{itemize}
		\item{Seek Time}
		\begin{itemize}
			\item{Die Zeit um den Lesekopf auf die richtige Spur zu bewegen}
		\end{itemize}
		\begin{itemize}
			\item{Rotational Latency}
			\begin{itemize}
				\item{Die Zeit die benötigt wird damit der relevante Sektor den Lesekopf erreicht.}
			\end{itemize}
			\item{Lesezeit}
			\begin{itemize}
				\item{Die Zeit die benötigt wird um die Datei auszulesen}
			\end{itemize}
		\end{itemize}
	\end{itemize}
	Die Latenz wird als Summe dieser drei Faktoren berechnet. \\
	Die Seek Time wird aus der Drehzahl berechnet, wobei man den worst und average case beachtet. Abhängig von der Drehzahl kann man die maximale Zeit mit $L=60/Drehzahl$ berechnen. \\
	Ein weiteres Qualitätskriterium ist die Mean Time Before Failure (MTBF) was die statistische Wahrscheinlichkeit, dass die Festplatte ausfällt, darstellt. \\
	\subsubsection{Sektoren}
	Zusammenhängende Reihen auf Festplatten werden Blöcke genannt. Abhängig von der Größe der Festplatte kann es zu unbeschreibbaren Lücken zwischen den Blöcken kommen (Interblock Gaps). Diese Blöcke werden in der Regel mit einer Größe von 4096 Byte generiert. Dies hat einen historischen Grund, da 4KB eine Größe war, welche von jedem Betriebssystem unterstützt wurde. Da Programme mit der Zeit immer größer werden, wird es auch relevanter die Größe der Speicherblöcke zu erhöhen.
	\subsection{Solid State Drive (SSD)}
	Daten auf der SSD werden auf Seiten abgelegt, welche ebenfalls in Blöcke unterteilt sind. Dadurch spiegelt es HDDs wieder und man merkt so keinen Unterschied ob man eine HDD oder SSD verwendet (Außer bei Schreibgeschwindigkeiten). Man muss beachten, dass SSDs keinen Block neu beschreiben können sondern stets einen Block löschen muss um ihn danach komplett neu zu beschreiben. Da die Lebensdauer einer SSD anhand ihrer Löschvorgänge beschränkt ist, werden zu löschende Daten in Erase Blocks gebündelt um dann zusammen gelöscht zu werden. Wenn man ein File also 'Updated' wird in der SSD der Speicherblock an einen anderen Ort verschoben und der alte Block zum Löschen vorgemerkt.
	\subsubsection{Wear Levelling}
	Eine Methode zur Verlängerung der Lebensdauer einer SSD ist das Wear Levelling. Dabei wird gespeichert wie oft ein Block bereits gelöscht worden ist und anhand dieser Information werden Erase Blocks erstellt. So kann man sicherstellen, dass alle Speicherblöcke gleichermaßen benutzt werden um so nicht einen Teil der SSD schnell zu verlieren.
	\subsubsection{Praxis}
	Die Performance einer SSD hängt von vielen internen Effekten ab. Eine volle SSD ist bedeutend langsamer als eine leere. Hohe Temperaturen können zu geringeren Geschwindigkeiten führen. SSDs sind auch bedeutend schneller bei sequenziellem als bei zufälligem Zugriff. Im schlimmsten Fall kann eine SSD langsamer sein als eine HDD.
	\subsection{Zugriffsoptimierung}
	Transfer zwischen persistentem Speicher und RAM sollte möglichst effizient sein. Dafür ist das Datenbank Management System (DBMS) verantwortlich. Dieses kann so versuchen die Anzahl der Zugriffe zu minimieren sowie so wenige Blöcke wie möglich zu schreiben/lesen. Das DBMS will auch so viel Speicher wie möglich innerhalb des RAMS behalten. \\
	Man kann jedoch auch innerhalb des Zugriffs in der Festplatte optimieren um so einen besseren Block Speicherzugriff zu erzielen. Mittels Disk Arm Scheduling kann man sicherstellen, dass bei einem Zugriff der durchgeführte Weg des Lesearms so gering wie möglich ist. Das passiert mittels Elevator-Algorithmus, welcher bei einem Pfad die geringste Distanz findet. Zusätzlich kann man bereits bei der Speicherung der Daten diesen so abspeichern, dass er beim Lesen effizient ausgelesen werden kann. Mit der Zeit geschieht jedoch stets eine Fragmentierung des Speichers, sodass die Daten erneut optimiert werden müssen. \\
	Alternative Wege um den Durchsatz zu erhöhen sind asynchrone Optimierungsvorgänge. So kann der Controller auf die Platte schreiben, wenn gerade Ressourcen zur Verfügung stehen um diese später schneller wieder finden zu können.
	\subsection{Logging}
	Das Logging schreibt ständig den Status sowei Vorgänge der Datenbank auf einen Speicher. Hierfür gibt es spezieller Logdisks, welche sequentiell beschrieben werden um so den Speicher zu optimieren.
	\subsection{Puffer}
	Der Puffer sitzt zwischen der Festplatte und dem Hauptspeicher und kann für schnellerem Zugriff oft benützter Dateien verwendet werden:
	\begin{itemize}
		\item{Anforderung an Puffer wird verwertet}
		\item{Falls der Block im Puffer nicht verfügbar ist: Speicher wird für Block reserviert.}
		\item{Block wird aus Festplatte gelesen und in Puffer zwischengespeichert und überschrieben sobald er sich verändert.}
	\end{itemize}
	Es gibt verschiedene Strategien um Blöcke zu löschen, falls kein Speicher mehr vorhanden ist:
	\begin{itemize}
		\item{Least Recently Used (LRE)}
		\begin{itemize}
			\item{Der am längsten nicht verwendete Block wird ersetzt. Durch diese können Zugriffsmuster zur Abschätzung der Zukunft verwendet werden.}
		\end{itemize}
		\item{Most Recently Used (MRU)}
		\begin{itemize}
			\item{Der als letztes verwendete Block wird verworfen.}
			\item{Diese Strategie kann fatal für Datenbanken sein, da so kein Puffer für oft verwendete Dateien erstellt wird.}
		\end{itemize}
		\item{Pinned Block}
		\begin{itemize}
			\item{Gibt einen Block an, welcher nicht gelöscht werden darf}
		\end{itemize}
		\item{Toss Immediately}
		\begin{itemize}
			\item{Jegliche Daten werden direkt nach Verwendung verworfen.}
		\end{itemize}
	\end{itemize}
	\subsection{Dateiorganisation}
	Auch die Organisation der Daten selbst kann ein Faktor für die Zugriffszeiten sein. Daten sind in ihrer grundlegendsten Form nur eine Sequenz an Blöcken, wodurch man ihre Abspeicherung anhand der spezifischen Anforderungne optimieren kann.
	\subsubsection{Cooked vs Raw}
	In einem DBMS kann man entweder den Controller des Betriebssystems verwenden um Daten abzuspeichern oder selbst einen definieren. Dies nennt man entweder cooked oder raw. Cooked ist zwar einfach zu implementieren, kann jedoch womöglich unomptimiert sein. Raw lässt zwar bedeutend mehr Optimierung zu, die gesamte Abspeicherung obliegt jedoch der Datenbank wodurch es viel Aufwand ist.
	\subsubsection{Variable vs Fixed Length}
	Die Anordnung einer Datei kann entweder fixiert geschehen (Wenn x byte an Daten sequenziell existieren) oder eine variable Länge besitzen. Wenn die Datensatzlänge fixiert ist, muss man, falls eine Datei in der Mitte gelöscht wird, damit umgehen, wofür es drei Strategien gibt:
	\begin{itemize}
		\item{Alle Datensätze werden um eine Position nach oben verschoben}
		\item{Der letzte Datensatz wird in die Lücke kopiert}
		\item{Jeder Datensatz hat eine Referenz auf den nächsten Datensatz (Free List)}
	\end{itemize}
	Heutzutage wird größtenteils die Free List Strategie verwendet da man so den geringsten Performanceverlust hat. \\
	Speziell bei varchars, welche eine variable Größe haben können, ist eine flexible Datenlänge jedoch von Nutzen. [TODO Slide 40 - 48]
	\section{Logische Anordnung}
	Die logische Anordnung von Datensätzen in der Datenbank kann auch ein großer Faktor für dessen Zugriffszeit sein. Dabei gibt es in der Regel drei Ansätze zur Datenspeicherung: Sequentiell, Heap oder Hash.
	\subsection{Sequentiell}
	Die Datensätze sind in der sequentiellen Anordung nach ihrem Suchschlüssel geordnet. Der Suchschlüssel ist in der Regel der Primary Key. Diese Datensätze sind jeweils mit Pointern verkettet. Idealerweise sind diese Datensätze nicht nur logisch (nach den Pointern) sondern auch physisch (im Speicher selbst) sortiert gespeichert. Das hat den Grund, dass es schneller ist Datensätze auszulesen wenn diese hintereinander liegen. Solch eine sequentielle Anordnung ist jedoch nach einer Zeit nicht mehr zu 100\% durchführbar. Wenn man zum Beispiel ein Element löscht muss man viele Elemente neu anordnen um die gleiche Sortierung zu erhalten. Ein noch größeres Problem ergibt sich, wenn man neue Daten zwischen den alten einfügen will, der Block jedoch gefüllt ist. Dann muss die neue Datei in einen Overflow-Block verschoben werden, wodurch die physische Sortierung nicht mehr gegeben ist. Alternativ kann die Datenbank in ihrer Gesamtheit in einen neuen Block verschoben werden, das kann jedoch extrem aufwändig sein. \\
	Eine sequentieller Speicherung kann so aussehen. Dabei ist zu beachten, dass der Verweis stets auf das nächste Element und im Fall des letzten Elements nirgends hin verweist: \\
	\begin{tabular}{| l | l | l | l | l |}
		\toprule
		\# Datensatz & KontoNr & Filiale & Kontostand & Verweis \\ \midrule
		record 0 & K-1234 & Graz & 450 & record 1 \\ \hline
		record 1 & K-2345 & Wien & 750 & record 2 \\ \hline
		record 2 & K-9022 & Salzburg & 248 & record 3 \\ \hline
		record 3 & K-3748 & Graz & 876 & record 4 \\ \hline
		record 4 & K-8473 & Linz & 230 & record 5 \\ \hline
		record 5 & K-9876 & Klagenfurt & 345 & <leer> \\
		\bottomrule
	\end{tabular}
	\subsubsection{Statisches Hashing}
	Statisches Hashing ist eine spezielle Form der Dateiorganisation. Dabei werden die Datensätze in 'Buckets' gespeichert, welche danach eine konstante Zugriffszeit ermöglicht. Ein Bucket ist eine Speichereinheit welche aus mehreren Blöcken auf der Festplatte besteht und Datensätze mit bestimmten Suchschlüsseln beinhaltet. Die Schlüssel muss dabei jedoch nicht die selben sein. Das funktioniert, da eine Hashfunktion aus unterschiedlichen Inputs den gleichen Output generieren kann. Ein sehr einfaches Beispiel einer solchen Hashfunktion wäre Modulo. Da diese nur den Rest, jedoch nicht das Ergebnis speichert, kann es trotz unterschiedlicher Operation zum gleichen Rest kommen, welche dadurch gebündelt werden. \\
	Das praktische an einer solchen Hashfunktion ist, dass das Hashing eine konstante Zeit benötigt und so unabhängig von der Position die gleiche Zeit benötigt. Bei sehr großen Datensätzen kann eine Hashfunktion etwas länger als konstante Zeit benötigen da der Algorithmus danach immer noch eine große Menge an Daten durchsuchen muss (Aber in keinster Weise die gesamte.) 	Idealerweise sollte eine Hashfunktion die gleiche Menge an Werten in alle Buckets verteilen und diese so zufällig wie möglich zuweisen. Also sollte bei 10 Buckets nicht einer die Hälfte aller Werte besitzen und auch nicht ab einem gewissen Wert einem dieser Buckets mehr zuweisen. \\
	Die Performance einer Hashfunktion muss jedoch auch immer abwägen wie viel Rechenleistung pro Hash nötig ist, da der zwar konstante Wert trotzdem einen größeren Aufwand als nötig bedeuten kann.
	\paragraphlb{Bucket Overflow}
	Ein Bucket kann jedoch auch voll werden, in welchem Fall ein Bucket Overflow entsteht. Das kann mehrere Gründe haben: Entweder es gibt zu wenige Buckets oder es besteht eine ungleichmäßige Verteilung der Buckets. Während ein einfacher Weg wäre die Größe oder Menge der Buckets zu erhöhen, ist in der Regel eine ungleiche Verteilung der Grund für einen Overflow, weshalb man versuchen sollte einen anderen Algorithmus zu verwenden.
	\subparagraphlb{Closed Addressing (Overflow Chaining)}
	Ein Overflow kann jedoch nie komplett vermieden werden, weshalb man Overflow Chaining betreiben kann. Hierbei wird, falls ein Bucket einen Overflow erlebt, der letzte Wert durch einen Pointer auf einen neuen Pointer ersetzt, welcher wiederum neue Werte enthält. Dieser neue Bucket hat jedoch immer noch den gleichen Hashwert als der ursprüngliche Bucketm weshalb er als Closed Addressing bezeichnet wird.
	\subparagraphlb{Open Addressing}
	Alternativ dazu besteht das Open Addressing bei der die Menge der Buckets fix ist und bei einem Overflow wird dieser in einen anderen, bereits existierenden Bucket überführt. Diese Strategie findet jedoch in Datenbanken nur selten Anwendung da das Löschen von Einträgen sich als außerst schwierig gestaltet.
	\paragraphlb{Beispiel:}
	Ein Beispiel für statisches Hashing wäre die Organisation einer Relation mit dem Standort als String als Suchschlüssel. Dabei wird der Modulo 10 der Summe der numerischen Werte der Buchstaben als Bucket verwendet. So können gleichzeitig 10 Buckets existieren wodurch idealerweise nur ein zehntel der Datenbank durchsucht werden muss. Wenn der Suchschlüssel also \texttt{Linz} ist, muss man \texttt{l + i + n + z -> 12 + 9 + 14 + 26 = 61} rechnen, dieses Ergebnis durch 10 dividieren und dessen rest nehmen: \texttt{61 \% 10 = 1}. Also würde Linz in den Bucket Nummer 1 gehen.
	\subsection{Dynamisches Hashing}
	Eine Alternative zum statischen Hashing ist das dynamische Hashing. Der Umstand, dass es sich verändern kann hat einen wichtigen Vorteil, da die Größe einer Datenbank sich ständig verändert. Am Anfang eines Lebens einer Datenbank ist diese noch sehr klein und wird eventuell nie größer, die Veränderung wenn man viele Daten hat ist jedoch extrem aufwändig. Beim dynamischen Hashing kann man die Menge der Buckets dynamisch bestimmen. Dabei gibt es wiederum zwei Wege dies zu vollbringen: Das reguläre dynamische Hashing oder das erweiterbare Hashing, was eine spezielle Form des dynamischen Hashings darstellt.
	\subsubsection{Erweiterbares Hashing}
	Bei dem erweiterbaren Hashing berechnet die Funktion den Hashwert für eine sehr große Menge an Buckets. Man kann zum Beispiel eine 32-Bit Zahl verwenden um so 4 Milliarden Buckets zu definieren. Bei dem Hashing wird jedoch nur ein Teil des Hashwerts verwendet. So kann man nur den niederwertigsten oder höchstwertigsten Bit verwenden um den Bucket zu wählen. So kann man genau definieren welche Buckets Verwendung finden, man kann die Menge jedoch stets erhöhen. Bei der Abspeicherung wird anhand der Anforderungen ein Teil des Hashwerts verworfen und danach anhand einer Tabelle ein Bucket zugewiesen. Wenn nun jedoch ein Bucket voll ist, kann man diesen in zwei Teile spalten (Bucket Split). Danach werden die Inhalte neu gehasht und in die neuen Buckets verteilt. Was man hierbei jedoch beacthen muss, ist, dass die globale Tiefe (Die Tiefe der Verteilungstabelle) nie niedriger sein darf als die eines lokalen Buckets (Die Tiefe der Buckets selbst)\\
	Wenn die Werte in der Datenbank sich verringern, können Buckets auch wieder verschmolzen werden. Dazu müssen zwei Buckets den gleichen um 1 verringerten Bitwert haben. (Also können zwei Buckets mit den Werten 11 und 01 verschmolzen werden, aber nicht zwei Buckets mit den Werten 11 und 10) Zusätzlich dürfen sich in beiden Buckets keine Werte mehr befinden. Danach kann man die Buckets verbinden und die Pointer neu verteilen. Wenn dieser verschmolzene Bucket als einziger noch die größere Tiefe hatte, kann man danach auch die globale Tiefe um eins verringern. \\
	Der große Vorteil dieses Ansatzes ist, dass Datenabruf relativ günstig ist, da die Menge der Buckets stets eine akzeptable Größe haben. Man muss auch die Datenbank nie neu verteilen, da nur lokal neue Werte generiert werden müssen. Es kann jedoch passieren, dass viele Werte in einen Bucket gegeben werden, was die lokale Größe der Buckets negativ beeinflussen kann. Zusätzlich ist die Größe der Buckets statisch wodurch man selbst bei einer kleinen nötigen Vergrößerung diesen Bucket Split durchführen muss.
	\paragraphlb{Beispiel:}
	Wenn man also beispielweise 7 Werte hat (16, 4, 24, 6, 22, 10, 31) und die globale Tiefe 1 ist, wird nur das LSB für die Buckets herangezogen. Es wird angenommen, dass ein Bucket nur 3 Werte gleichzeitig aufnehmend kann. Die Bitwerte dieser Zahlen sind also (10000, 00100, 11000, 00110, 10110, 01010, 11111) und es gibt zwei Buckets, einen mit dem Wert 0 und einen mit dem Wert 1. Wir sehen, dass bereits die ersten 4 Werte zu einem Bucket Overflow führen. Aus diesem Grund wird die lokale Tiefe des Bucket 0 auf 2 erhöht. Da die globale Tiefe noch 1 ist, muss diese auch auf 2 erhöht werden. Aus diesem Grund hat die Tabelle nun 4 Möglichkeiten zur Verteilung: 00, 01, 10 und 11. Die drei Werte welche bereits in dem 0 Bucket waren müssen nun neu gehasht werden, wobei jeweils die letzten zwei Bit zur Evaluierung herangezogen werden. Da alle drei wiederum 00 als letzte Bits haben werden sie in den Bucket 00 gegeben. Das vierte Element hat jedoch den Wert 10 und kommt deshalb auch in den Bucket 10. Das selbe passiert mit den nächsten zwei Werten. Nur der siebte Wert 11111 hat eine 1 am Ende und kommt deshalb in einen anderen Bucket. Da die lokale Tiefe des 1 Buckets immer noch 1 ist, verweisen beide Buckets 11 und 01 auf den selben Bucket.
	\subsubsection{Katalog}
	Zusätzlich zur Speicherung der Daten selbst gibt es den Datenbankkatalog. Dieser speichert stets Metainformation der Datenbank wie den Namen der Relation, die Attribute und Typen jeder Relation sowie Integritätsbedingungen. Gleichzeitig verwaltet es auch die Berechtigungen der einzelnen Benutzer um zu kontrollieren wer die Datenbank verändern oder neue Rechte vergeben darf. Es gibt zusätzlich noch weitere Information wiie die Anzahl der Attribute sowie ein Cache für oft verwendete Werte. \\
	Die physische Dateiorganisation wird auch hier gespeichert wie den Speicherform der Relation sowie deren phsysischer Speicherort und die Adresse im Speicher des ersten Blocks. \\
	Der Katalog selbst hat auch ein spezielles Schema um diesen für effizienten Zugriff zu optimieren. So werden die Relationen, Attribute Benutzer sowie Index und View in einem speziellen Format abgespeichert: \texttt{RELATION-METADATA (\underline{relation\_name}, nr\_of\_attributers, storage\_organization, location)}
	\section{Index}
	Ein Index ermöglicht einen beschleunigten Zugriff auf eine Datenbank, änhlich zum Verzeichnis eines Buches. Eine Index Datei besteht in der Regel aus der Form des Indexeintrags, einem Schlüssel welcher zum Finden benötigt wird, sowie ein Pointer auf den Datensatz selbst. Es ist wichtig, dass ein Index stets kleiner sein muss, als die Datenbank selbst, da man sonst keinen Performancegewinn hat. \\
	In einem Index können mehrere Arten von Anfragen geschehen: Eine Punktanfrage sucht anhand eines Primärschlüssels (z.B. \texttt{SVN=1988-2018}) und liefert so immer nur einen Eintrag. Eine Mehrpunktanfrage übergibt einen Wert, kann jedoch zu mehr als einem Rückgabeeintrag führen (z.B. \texttt{alle Personen die 1980 geboren worden sind}) und eine Bereichsanfrage efragt Einträge anhand eines Bereichs an Kriterien (z.B. \texttt{Personen die mehr als 100.000€ verdienen})
	\subsection{Indextypen}
	Indextypen unterscheidet man nach verschiedenen Kriterien:
	\begin{itemize}
		\item{Die Ordnung der Index Datei}
		\begin{itemize}
			\item{Clustering Index}
			\item{Non-Clustering Index}
		\end{itemize}
		\item{Die Art der Indexeinträge}
		\begin{itemize}
			\item{Sparse Index}
			\item{Dense Index}
		\end{itemize}
	\end{itemize}
	Indextypen basieren auf einer Kombination dieser zwei Kriterien. Dabei sind jedoch einige Typen nicht möglich oder kommen nur selten vor. Ein Clustering Index ist zum Beispiel meistens sparse, während ein Non-Clustering Index \textit{immer} dense ist. \\
	Wahr für alle Indextypen ist, dass Punktanfragen signifikant schneller bearbeitet werden können. Dieser Fakt ist weniger wahr für Mehrpunktanfragen und Bereichsanfragen. Ein Index erzeugt jedoch auch Mehrkosten da bei jeder Änderung der Datenbank der Index angepasst werden muss.
	\subsubsection{Clustering Index}
	Bei einem Clustering Index ist sowohl die Index-, als auch die Datendatei, in einer nach dem Suchschlüssel sequentiell geordneter Form. Das macht sowohl Punkt- als auch Mehrpunkt- und Bereichsanfragen sehr effizient und lässt einen guten Nicht-Squentiellen Zugriff zu. Großer Nachteil ist, dass man \textit{nur einen} Clustering Index pro Tabelle haben kann. Wenn man also einen Clustering Index auf eine Kontonummer hat, muss man für den Kontonamen einen Non-Clustering Index anfertigen. Wenn nur ein Clustering Index für eine Suche mit einem anderen Kriterium vorhanden ist, muss die gesamte Tabelle manuell durchsucht werden. Also sollte man einen Clustering Index für oft verwendete Werte verwenden und sonst Werte als Non-Clustering Index abspeichert.
	\subsubsection{Non-Clustering Index}
	Ein Non-Clustering Index ist in der Index Datei sequentiell geordnet, jedoch nicht bei den Daten selbst. So verweisen Pointer in dem Index zufällig auf Werte. Das macht Punktanfragen sehr schnell, da so nur einem Pointer gefolgt werden muss. Mehrpunkt und Bereichsanfragen können jedoch sehr rechenintensiv sein und besitzen nur eine gute Effizienz wenn sie kleine Teile der Datenbank abfragen. (Die Datenbank muss jedem Pointer einzeln folgen)
	\paragraphlb{Primär- Sekundärindex}
	In der Praxis unterscheidet man zwischen dem Primärindex und dem Sekundärindex. In der Regel bedeutet das nur, dass der Primärindex der Clustering Index ist und der Sekundärindex andere Non-Clustering Indexe sind.
	\subsubsection{Sparse Index}
	Ein Sparse Index hat einen Index Eintrag welcher auf mehrere Datensätze verweist. So kann man die Größe des Index verringern. Ein Sparse Index ist jedoch nur möglich wenn man einen Clustering Index hat, da ein Sparse Index nur funktioniert wenn der Verweis auf einen Eintrag sequentiell abarbeitbar ist. Ein Clustering Index kann jedoch auch Dense sein.
	\subsubsection{Dense Index}
	Im Dense Index hat jeder Index-Eintrag einen eindeutig zugewiesenen Ort in den Daten. Aus diesem Grund ist ein Dense Index bedeutend größer als ein Sparse Index, ist in der Regel jedoch trotzdem kleiner als die Daten selbst da der Index ja nur die Pointer und nicht die Daten selbst beinhaltet. Aus diesem Grund muss ein Non-Clustering Index immer Dense sein. Ein Vorteil eines Dense Index ist, dass eventuell eine Datenbank gelesen werden kann, ohne dass direkt auf diese zugegriffen werden muss. Das nennt man dann einen 'Covering Index'.
	\subsection{Duplikate im Index}
	Wenn in einem Index Einträge doppelt vorhanden sind kann dessen Handhabung relativ schwer sein. Das ist vor allem im Falle von $B^+$ Baum Indizes der Fall ($B^+$ Bäume können anders als Binärbaume viele Kinder pro Knoten haben. (Siehe Datenstrukturen und Algorithmen)) \\
	Man kann damit jedoch umgehen indem man Mehrere Indexeinträge einführt. Zusätzlich kann es ein Problem bei Buckets sein, da dann ein Eintrag im Index zu mehreren möglichen Bucketeinträgen führt. Schlißelich muss man den Suchschlüssel noch eindeutig machen. Dabei kann ein Tupel Identifier (TID) helfen um gleiche Datensätze eindeutig zu machen. Gleichzeitig kann man auch aus einer Kombination des TID und dem Bucket einen eindeutigen Key definieren. \\
	In der Praxis werden jedoch doppelte Einträge als solche behandelt und diese auch zurückgegeben. Im Rahmen dieser Lehrveranstaltung wird jedoch angenommen, dass alle Datensätze immer eindeutig sind, da manche besprochenen Inhalte sonst nicht möglich sind.
	\subsection{ISAM Index}
	Da ein großer Index sehr teuer ist, ist es oft vonnöten einen mehrstufigen Index einzuführen. Da der Index selbst sequenziert wird, ergibt es einen neuen Index, welcher auf den Index verweist. Ein solcher Index besteht immer aus einem äußeren und einem inneren Index. Der äußere Index verweist dabei auf die Index Datei, während der innere Index auf die Daten selbst verweist. Falls der äußere Index zu groß wird, wird danach eine neue Index Ebene eingeführt. Diese Art des Indexing bezeichnet man als Index Sequential Access Method (ISAM).
	\subsubsection{CRUD}
	Zum Durchsuchen eines ISAM-Index muss man im Wurzelknoten beginnen und alle Pointer welche zu passenden Ergebnissen führen die Pointer durch die Indizes verfolgen. Beim Einfügen, Löschen und Update muss man jedoch alle möglichen Indexknoten durchlaufen da man so sicherstellen kann, dass die Integrität bewahrt wird.
	\subsection{\texorpdfstring{$B^+-Baum$}{B+ Baum}}
	Eine Alternative zu ISAM ist die Anordnung eines Index als $B^+-Baums$. Ein $B^+-Baum$ passt seine Anzahl der Ebenen stets automatisch an. Das nennt man selbstständige Reorganisation und wird nach einem Lösch- oder Einfügevorgang nötig. Aus diesem Grund ist die gesamte Reorganisation des Index nicht erforderlich. Das bedeutet jedoch natürlich auch, dass $B^+-B\textnormal{ä}ume$ einen potentiellen Merhaufwand besitzen und bedeutend komplexer in der Implementierung ist. Bei einem Einfüge- oder Löschvorgang muss auch mehr Rechenleistung aufgebracht werden, da der Baum angepasst werden muss. \\
	Ein $B^+-Baum$ ist ein 'hohler' Baum, hat also nur in den Blättern (Elemente ohne Kindelemente) echte Daten. Alle Zweige (Elemente mit Kindelementen) dienen nur zum richtigen Blatt zu finden. Gleichzeitig sind alle Werte in diesen Blättern sequentiell gespeichert und miteinander verbunden, wodurch man von einem Blatt die Position weiterer Blätter ableiten kann und mit dieser Information die Blätter auch durchstreifen kann. \\
	\subsubsection{Eigenschaften}
	Zusammengefasst hat ein $B^+-Baum$ vier Eigenschaften:
	\begin{enumerate}
		\item{Jeder Weg von der Wurzel zu einem Blatt hat die selbe Länge}
		\begin{itemize}
			\item{Das ist wichtig, da so die Zugriffszeit für jegliche Anfrage dieselbe ist. Ansonsten wären Laufzeitberechnungen nur schwer durchführbar.}
		\end{itemize}
		\item{Jeder Knoten hat mindestens k und höchstens 2k Einträge. Blätter haben mindestens $k^*$ und maximal $2k^*$ Einträge. Die Wurzel hat maximal 2k Einträge, oder $2k^*$ falls die Wurzel ein Blatt ist. (Also kann die Wurzel weniger als k Einträge haben.)}
		\begin{itemize}
			\item{Die Verhältnisse aller Knoten- sowie Wurzelverbindungen müssen gleichmäßig verteilt sein.}
		\end{itemize}
		\item{Jeder Knoten mit n Einträgen hat n+1 Kinder}
		\begin{itemize}
			\item{Der Fan-Out eines Knotens beschreibt stets die Menge an Entscheidungen die in der Struktur getroffen werden kann. Der Fan-Out eines Knotens ist immer eines mehr als die Menge an verbundenen Knoten.}
		\end{itemize}
		\item{Enthält $R_1$ bis $R_n$ Knoten zur Verfügung wobei jeder Knoten zwischen k und 2k Kinder hat}
	\end{enumerate}
	Diese Eigenschaften garantieren einige Vorteile und Nachteile:
	\begin{itemize}
		\item{Da jeder Knoten k Einträge benötigt, ist jeder Knoten zu 50\% gefüllt.}
		\item{Obwohl jedes Lesen dadurch wahrscheinlich einen random access der Festplatte verursacht, sollte durch die k bis 2k Vorgabe die Tiefe Flach bleiben um Zugriffe zu verringern.}
		\item{Die ersten Ebenen werden oft im RAM gecached um die Zugriffszeiten zu verringern.}
	\end{itemize}
	\subsubsection{Balancing}
	Der $B^+-Baum$ ist wie bereits erwähnt in der Lage sich selbst zu reorganisieren. Dabei muss man bei jedem Einfügen und Löschen die Integritätsbedingung des Baumes überprüfen. Wenn ein Baum also ein k von 2 hat und ein weiterer Knoten in eine Tabelle mit bereits vier Einträgen hinzugefügt wird, dann muss diese Tabelle gesplittet werden. Dazu wird die Tabelle in zwei Tabellen mit jeweils der Hälfte an Werten aufgeteilt und die Pointer in der Tabelle darüber gesplittet. Dabei muss man jedoch sicherstellen, dass das Splitting der Tabellen nicht wiederum zu einer Integritätsverletzung führt und die obere Tabelle im Fall ebenfalls splittet. \\
	Das umgekehrte passiert beim Löschen. Wenn ein Knoten gelöscht wird, muss man immer sicherstellen, dass 2 Knoten in einer Tabelle verbleiben und 
	\subsection{Hash Index}
	Ein Hash Index erlaubt eine effizientere Suche und gleicht die Nachteile von ISAM und dem $B^+-Baum$ aus, da diese stets eine binäre Suche und eine Indexsuche durchlaufen müssen. Indem man Daten hasht, kann man direkt auf die Daten zuzugreifen ohne eine Indexstruktur verwenden zu müssen. Natürlich kann man Hashing auch verwenden um einen Index zu erstellen. \\
	Spezifisch organisiert ein Hash Index einen Suchschlüssel aus Paaren an Pointern mit der gesuchten Datei.
	\subsubsection{Nachteile}
	Obwohl ein Hash Index im Mittel sehr schnell ist, hat er einen großen Nachteil: Im Worst Case hat ein Hash Index eine Komplexität von O(n). Das passiert wenn ein Hash Index sehr viele identische Werte enthält, da viele Werte in den gleichen Bucket kommen und so den Vorteil des Hashings nicht nutzen. \\
	Ein $B^+-Baum$ hat andererseits dank seiner konstanten Länge stets die Komplexität von O(log(n)). Also ist ein Hash Index im besten Fall schneller als ein $B^+-Baum$ im schlechtesten Fall jedoch bedeutend schlechter. \\
	Im Allgemeinen kann man entscheiden, welcher Index verwendet werden soll, indem man die Art der Anfragen ansieht. Bei Punkt- und Mehrpunktanfragen haben ein $B^+-Baum$ und ein Hash Index in der Regel annähernd gleiche Geschwindigkeiten, wodurch man beide verwenden kann. Wenn man jedoch viele Bereichsanfragen (Wie alle Konten mit einem Kontostand zwischen x€ und y€) hat, dann sollte man definitiv einen $B^+-Baum$ verwenden da dieser die Sequentielle Sortierung der Daten verwenden kann um alle Daten in einem Schwung zu finden.
	\subsection{Mehrschlüsselindex}
	Bei einer Anfrage mit zwei Werten (Wie einem SELECT wobei im WHERE zwei Attribute verundet werden) gibt es verschiedene Strategien diese abzuarbeiten.
	\begin{enumerate}
		\item{Den ersten Wert suchen und diesen mit dem zweiten Wert vergleichen}
		\item{Den zweiten Wert suchen und diesen mit dem zweiten Wert vergleichen.}
		\begin{itemize}
			\item{In den ersten beiden Ansätzen hat man stets eine schnelle und eine langsame Anfrage. Aus diesem Grund sollte man das kleinere Datenset zuerst suchen und dann mit dem größeren Vergleichen um die Anfrage effizienter zu gestalten.}
		\end{itemize}
		\item{Wenn beide Werte einen Index besitzen kann man zwei separate Abfragen mit beiden Werten machen und die Schnittmenge der Ergebnisse berechnen. Dies kann signifikant schneller sein, als die ersten zwei Strategien.}
		\begin{itemize}
			\item{Wenn jedoch eine der beiden Werte bedeutend öfter vorkommt als der andere, kann diese Strategie relativ langsam sein.}
		\end{itemize}
	\end{enumerate}
	Es gibt jedoch auch weitere spezialisierte Indizes für kombinierte Schlüssel. Einige Beispiele sind Grid Files, Quad-Trees und Bitmap-Indizes 
	\subsection{Indizes in SQL}
	Der SQL Standard 92 definiert keine Syntax für Indizes, da es kein Teil des logischen Datenmodells ist. Alle gängigen Datenbanksysteme stellen jedoch ihre eigene Indexsysteme zur Verfügung. In der Regel sieht die Syntax wie folgt aus:
	\begin{verbatim}
	CREATE INDEX name ON RELNAME | ATTRLIST -> CREATE INDEX myINDEX ON
	\end{verbatim}
	Wenn eine Datenbank einen neuen Wert einfügt oder löscht, wird stets der Index angepasst. Dadurch kann die Existenz eines Indexes Änderungsoperationen verlangsamen. \\
	Das erste Erstellen eines Indexes kann abhängig von der Größe der Datenbank auch relativ lange dauern. Da die DB einen Index anhand der Struktur erstellt, ist es auch bedeutend effizienter einen Index erst nach Einfügen von Daten anzupassen anstatt bei jedem Einfügen
	\section{Zugriffsoptimierung}
	Eine der wichtigsten Aufgaben des Datenbankmanagementsystems (DBMS) ist ein effizienter Auswertungsplan einer query. Der Ablauf einer Query passiert stets in einer festen Abfolge:
	\begin{enumerate}
		\item{Query wird eingelesen}
		\item{Query wird in relationale Algebra übersetzt}
		\item{Auswertungsplan wird erstellt}
		\begin{itemize}
			\item{Wenn möglich wird die Query optimiert um die Anfragenlaufzeit zu reduzieren. (z.B. Umdrehen des Kreuzprodukts)}
		\end{itemize}
		\item{Query wird ausgewertet}
	\end{enumerate}
	Der Optimierungsgrad der Anfrage kann dabei einen sehr großen Einfluss auf die Dauer der Zugriffszeit haben. Zusätzlich gibt es auch noch weitere Hardwarebasierte Faktoren:
	\begin{itemize}
		\item{Stärke der CPU}
		\item{HDD Zugriffszeiten (Sequentiell oder Random)}
		\item{Puffergröße (RAM)}
		\item{Geschwindigkeit des Netzwerks}
	\end{itemize}
	\subsection{Puffergröße}
	Ein größerer RAM reduziert die Anzahl der Zugriffe auf die HDD, da mehr zwischengespeichert werden kann, womit dieser einer der größten Faktoren der Zugriffszeit ist. Die tatsächlich benötigte Puffergröße hängt von anderem am Server ausgeführten Prozesse aus und es kann schwer sein dies im Vorhinein akkurat abzuschätzen. In der Abschätzung der Anforderungen sollte immer vom Worst Case ausgehen, da es überall laufen muss, und nur besser läuft, falls das System besser ausgestattet ist.
	\subsubsection{Kosten eines Zugriffs}
	Die Kosten eines Plattenzugriffs hängt von drei Faktoren ab:
	\begin{itemize}
		\item{Dem Spurwechsel - Der mittleren Spurwechselzeit}
		\item{Der Block-Lese-Operation}
		\item{Der Block-Schreib-Operation}
	\end{itemize}
	Das Schreiben des Ergebnisses kann in der Regel vernachlässigt werden.
	\paragraphlb{Sortierung}
	Da diese drei Faktoren davon abhängen an wie vielen Orten nach den Daten gesucht wird, ist die Sortierung des Datensatzes extrem wichtig. Viele der Datenbankoperationen wie Joins sind wesentlich effizienter, wenn eine Relation sortiert ist. Durch das gewonnene Wissen wenn eine Relation sortiert ist, können die meisten Anfragen bedeutend effizienter ausgewertet werden. \\
	Ein Sekundärindex kann dabei ein großes Problem für diese Optimierung darstellen, da dieser die Daten nur logisch und nicht physisch sortiert. \\
	Die zwei am häufigsten verwendeten Sortieralgorithmen bei Datenbanken, ist der Quicksort und der Mergesort. Welcher der beiden verwendet wird, hängt davon ab ob die Relation oder der Puffer größer ist. Hat der Puffer eine ausreichende Größe, sollte Quicksort verwendet werden und sonst Mergesort. Das hängt damit zusammen, dass Quicksort mehr Anfragen an die Relationen benötigt und so nur schneller ist, wenn man komplett im Hauptspeicher agieren kann.
	\subsection{Selektion}
	Die Selektion (Siehe Datenbanken 1) wählt Daten anhand einer Einschränkung aus. Dabei gibt es zwei Wege um diese Anfrage abzuarbeiten:
	\begin{itemize}
		\item{Sequentiell}
		\begin{itemize}
			\item{Die gesamte Datenbank wird durchsucht und jedes Tupel wird gelesen um jene Tupel zu finden die die Relation erfüllen.}
		\end{itemize}
		\item{Indexsuche}
		\begin{itemize}
			\item{Der Index wird verwendet um Tupel im voraus auszuwählen. Zum Beispiel der Index eines $B^+-Baums$.}
		\end{itemize}
	\end{itemize}
	Offensichtlicherweise ist der Zugriff schneller, wenn man einen Index hat, da man so nur einen Teil der Datenbank durchsuchen muss anstatt jede Relation durchgehen zu müssen.
	\subsubsection{Prädikate}
	Die Art der Anfrage hängt auch von dem verwendeten Prädikat ab:
	\begin{itemize}
		\item{Gleichheitsanfrage}
		\begin{itemize}
			\item{$\phi_{X=Z}$(R)}
		\end{itemize}
		\item{Bereichsanfrage}
		\begin{itemize}
			\item{$\phi_{X<Z}$(R) oder $\phi_{X>Z}$(R)}
		\end{itemize}
		\item{Konjunktive Selektion}
		\begin{itemize}
			\item{$\phi_{\theta_1\lor\theta_2\lor\theta_3\lor...\lor\theta_n}$(R)}
			\item{Eine Suche die nur aus Oder-Verbindungen besteht}
		\end{itemize}
		\item{Disjunktive Selektion}
		\begin{itemize}
			\item{$\phi_{\theta_1\land\theta_2\land\theta_3\land...\land\theta_n}$(R)}
			\item{Eine Suche die nur aus Und-Verbindungen besteht}
		\end{itemize}
	\end{itemize}
	\subsubsection{Auswertung}
	Abhängig von dem Prädikat, wird die Suche auf anderer Weise durchgeführt:
	\paragraphlb{Lineare Suche}
	Jeder einzelne Block wird gelesen und die Selektionsbedingung überprüft. Dieser Vorgang ist wahrscheinlich teuer (abhängig von der Größe der Datenbank) aber immer möglich. Die Dauer der Auswertung ist hierbei unahängig von Indexstrukturen, Sortierung, Daten oder der Art der Selektion. Die Geschwindigkeit der Anfrage kann durch Lesen von hintereinanderliegenden Blöcken beschleunigt werden, wobei die Festplatte dabei auch durch pre-fetching die wahrscheinlich nächste Datei zur Verfügung stellt, bevor sie benötigt wird.
	\subparagraphlb{Kosten}
	Die Kosten dieser Suche sind in der Regel relativ hoch. Im Worst Case muss die gesamte Datenbank durchlaufen werden und selbst im Average Case muss die halbe Datenbank durchlaufen werden.
	\paragraphlb{Binäre Suche}
	Falls die Datensätze nach dem gesuchten Wert sortiert sind, kann man die Binärsuche anwenden. Dabei wird stets die Mitte der Daten (Der Pivotpoint) gewählt und anhand der gefundenen Datei im linken oder im rechten Block weitergesucht. Also wird in jeder Iteration die Menge der noch zu suchenden Daten halbiert, weshalb die Suche der Daten sehr effektiv ist.
	\subparagraphlb{Kosten}
	Die Auffindung des ersten Tupels ist das teuerste mit einer Laufzeit von $\log(b) + 1$, da der gesamte $B^+-Baum$ durchlaufen werden muss. Danach, kann man den Baum sequentiell durchlaufen, wodurch die weiteren Anfragen sehr schnell sind.
	\subsubsection{Suchverhalten}
	Die Kosten einer Anfrage können anhand des verwendeten Index sowie des Suchprädikat ermittelt werden. Angenommen wird, dass ein $B^+-Baum$ verwendet wird.
	\begin{itemize}
		\item{Primärindex + Gleichheitsbedingung auf Suchschlüssel}
		\begin{itemize}
			\item{Liefert genau einen Datendatz.}
			\item{Kosten ist ein Durchlauf der Knoten + 1}
		\end{itemize}
		\item{Clustered Index + Gleichheitsbedingung auf Suchschlüssel}
		\begin{itemize}
			\item{[TODO]}
		\end{itemize}
		\item{Sekundärindex + Gleichheitsbedingung auf Suchschlüssel}
		\begin{itemize}
			\item{Falls der Suchschlüssel der Kandidatenschlüssel ist}
			\begin{itemize}
				\item{Liefert einen Datendatz}
				\item{Kosten ist ein Durchlauf der Knoten + 1}
			\end{itemize}
			\item{Falls der Suchschlüssel nicht der Kandidatenschlüssel ist}
			\begin{itemize}
				\item{Liefert potentiell viele Datensätze}
				\item{[TODO KOSTEN]}
				\item{Diese Anfrage kann sehr teuer werden da alle Daten auf verschiedenen Blöcken liegen kann. Es kann potentiell schneller sein alle Datensätze sequentiell zu durchsuchen}
			\end{itemize}
		\end{itemize}
		\item{Primärindex auf X + Bereichsanfrage}
		\begin{itemize}
			\item{}
		\end{itemize}
		\item{Sekundärindex auf X + Bereichsanfrage}
	\end{itemize}
	\paragraphlb{Pointersuche}
	Wenn viele Pointer verfolgt werden müssen, da man einen Sekundärindex verwendet, kann die Anfrage \textbf{sehr} langsam sein. Da jeder Datensatz auf einem anderen Block liegen kann und nicht sortiert sind, muss jeder im random access gesucht werden.
	\subparagraphlb{Bitmap Index Scan}
	Mit einem Bitmap Index Scan kann man diesen Vorgang in manchen Fällen beschleunigen. Dabei wird eine Bitmap verwendet um entweder mit 0 oder 1 anzuzeigen ob der gesuchte Wert existiert oder nicht und man so nahezu sequentiell darauf zugreifen kann. Diese Strategie kann praktisch sein, wenn der Suchschlüssel kein Kandidatenschlüssel ist.
	\subsection{Join}
	Beim Join werden zwei Tabellen miteinander kombiniert. Das geschieht entweder anhand eines Werts (Beim Theta-Join $\bowtie_\theta$) oder mit dem gleichen Attribut in der anderen Tabelle (Beim Natural-Join $\bowtie$). \\
	Ein Join ist sowohl kommutativ, als auch assoziativ. Außer bei der Ordnung der Attribute spielt die Reihenfolge der Werte keine Rolle (Wodurch $r\bowtie=\pi(s\bowtie)$ und $r\bowtie$)$\bowtie=r\bowtie(s\bowtie)$. Auch wenn das Ergebnis der Operationen austauschbar sind, kann es einen \textbf{großen} Unterschied in der Performance machen. Das hängt damit zusammen, dass wenn eine große mit einer kleinen Tabelle verbunden wird, bedeutend mehr falsche Tupel erstellt werden als umgekehrt. \\
	Ein paar Definitionen sind:
	\begin{itemize}
		\item{$r\bowtie$}
		\begin{itemize}
			\item{r ist die äußere Relation}
			\item{s ist die inner Relation}
		\end{itemize}
		\item{Kardinalität: Die absolute Größe des Join Ergebnisses}
		\begin{itemize}
			\item{$|r\bowtie_\theta$s|}
		\end{itemize}
		\item{[TODO]: Relative Größe des Join Ergebnisses}
		\begin{itemize}
			\item{[TODO FORMEL]}
		\end{itemize}
		\item{[TODO SELEKTIVITÄT]}
	\end{itemize}
	\subsubsection{Arten der Joins}
	Es gibt verschiedene Algorithmen zur Auswertung von Join Befehlen:
	\begin{itemize}
		\item{Nested Loop Join (Immer anwendbar)}
		\item{Block Nested Loop (berücksichtigt Blöcke)}
		\item{Indexed Nested Loop Join (Erfordert Index auf innere Relation)}
		\item{Merge Join}
		\item{Hash Join}
	\end{itemize}
	\paragraphlb{Nested Loop Join}
	Der Nested Loop Join ist immer anwendbar, da jedes Tupel mit jedem anderen Tupel verglichen wird, ist dadurch aber meist teuer.
	Algorithmisch bestehen zwei Schleifen, welche jedes einzelne Tupel mit jedem anderen Tupel vergleicht und die Auswertung nur hinzufügt wenn sie der Anforderung entspricht.
	\subparagraphlb{Kosten}
	Da die äußeren Tupel nur ein Mal gelesen werden, die inneren aber n Mal, ist die Ordnung der Tupel für die Laufzeit relevant. \\
	Im Worst Case passt nur ein Block je Relation in den Puffer. Die Kosten setzen sich so aus $b_r+n_r*b_s$ zusammen. Der Menge an Blöcken der äußeren Relation plus der Menge an Datensätzen der äußeren Relation Mal der Menge an Blöcken der inneren Relation. Der Nested Loop Join ist somit bedeutend effizienter wenn die äußere Relation die kleinere Relation ist. (Da für jeden Datensatz ein Block der inneren Relation geladen werden muss.) 
	Im Best Case ist die innere Relation komplett im Hauptspeicher geladen, wodurch der Zugriff auf diese extrem schnell ist. In diesem Fall ist die Laufzeit annähernd linear anhand der äußeren Relation, da nur diese auf den Sekundärspeicher zugreifen muss. \\
	Aus diesem Grund sollte man überprüfen ob die kleinere Relation in den Puffer passt und wenn nicht die kleinere Relation als äußere verwenden.
	\subparagraphlb{Beispiel:}
	Die Relationen Person und Auto sollen gejoint werden. Person hat 155.000 Datensätze auf 250 Blöcken, während Auto 965.000 Datensätze auf 700 Blöcken hat. \\
	Wenn man $Person\bowtie Auto$ rechnet: $250+155.000*700=108.500.250$ Blockzugriffe \\
	Wenn man $Auto\bowtie Person$ rechnet: $700+965.000*250=241.250.700$ Blockzugriffe \\
	Wenn die Blöcke von Person in den Puffer passen: $700+250=950$
	\paragraphlb{Block Nested Loop Join}
	Bei dem Block Nested Loop Join wird nicht jeder Datensatz mit jedem Block verglichen, sondern jeder Block mit jedem Block. Also wird ein Block der äußeren Relation geladen und mit einem Block aus der inneren Relation verglichen. Also wird jeder Block jeder Relation nur ein Mal gelesen was zu einer bedeutend effizienteren Laufzeit führt.
	\subparagraphlb{Kosten}
	Im Worst Case muss so jeder Block beider Tupel verglichen werden, anstatt jeder Datensatz mit jedem Block: $b_r+b_r*b_s$.
	Im Best Case ändert sich im Vergleich zur anderen Methode nicht viel: $b_r+b_s$
	\subparagraphlb{Beispiel:}
	Die Relationen Konten und Kunde sollen gejoint werden. Kunde hat 15.000 Datensätze und 500 Blöcke und Konten 5.000 Datensätze und 100 Blöcke. Da jedoch nur Blöcke verglichen werden ist die Menge der Datensätze irrelevant. \\
	Wenn man $Konten\bowtie Kunde$ rechnet: $100+100*500=50.100$ Blockzugriffe \\
	Wenn man $Kunde\bowtie Konten$ rechnet: $500+500*100=50.500$ Blockzugriffe \\
	Wenn die Blöcke von Person in den Puffer passen: $500+100=600$ Blockzugriffe
	\section{Stored Procedures}
	Stored Procedures (SP) sind vorgefertigte Funktionen die vom DBMS im Systemkatalog abgespeichert werden. Diese werden dann von dort bei Bedarf ausgeführt. Ein SP selbst ist ein normaler SQL Befehl. SP sparen nicht bei der Laufzeit am Server sondern dem Netzwerkbedarf, da SQL-Queries relativ lang und komplex sein können. Wenn dieser SQL-Befehl bereits am Server existiert, muss nur eine Referenz auf diese übertragen werden, wodurch Bandbreite gespart wird. Gleichzeitig kann das DBMS die Befehle bedeutend besser optimieren da es mehr Zeit hat. Wenn man eine solche SP benötigt muss man nur den Trigger am Server aufrufen um sie anzustoßen.
	\subsection{SQL}
	In SQL wird eine SP durch
	\begin{verbatim}
	 CREATE PROCEDURE procedure_name
	 AS
	 SELECT * FROM customers
	 \end{verbatim}
	 Eine SP kann danach mittels \texttt{EXEC} auf diese Stored Procedure zugreifen.
	 \begin{verbatim}
	 EXEC procedure_name
	 \end{verbatim}
	 \subsection{Trigger}
	 Trigger können zusätzlich zur Ausführung der SP auch bei Insert-, Update- oder Deleteoperationen ausgeführt werden um so schnell eine gewisse Funktion auszuführen. \\
	 Ein Trigger bezieht sich genau auf eine Tabelle und kann:
	 \begin{itemize}
	  	\item{Ereignisgesteuert aufgrund einer Datenmanipulation (Create, Update, Delete) aufgerufen werden.}
	  	\item{Zeitpunktgesteuert (Bevor oder nach einem auslösenden Element)}
	  	\item{Befehls- oder Datensatzorientiert im Bezug auf die ausgelöste Aktion sein}
	  \end{itemize} 
	  Pro Tabelle und pro Spalte kann man pro Kombination maximal einen Trigger definieren. \\
	  \subsubsection{SQL}
	  Ein Trigger wird mittels \texttt{CREATE TRIGGER} erstellt, wobei man die Tabelle sowie den Zeitpunkt angeben muss:
	\begin{verbatim} Eckige Klammern [] sind optional
		CREATE TRIGGER [schema_name].trigger_name -> Name des Schemas und triggers
		ON table_name -> Tabelle auf die sich der Trigegr bezieht
		AFTER {[INSERT], [UPDATE], [DELETE]} -> Definition der Event(s)
		[NOT FOR REPLICATION] -> Trigger wird nicht ausgelöst wenn Daten bei Replikation kopiert werden
		AS
	  {sql_statement}
	\end{verbatim}
	\section{Datenbankangriffe}
	Wenn man eine Datenbank unterhält muss man natürlich immer sicherstellen, dass unberechtigte Benutzer keinen Zugriff auf den kompletten Datensatz haben. Ein grundlegender Faktor für Datensicherheit sind Views (Siehe Relationale Datenbanken 1), da man so nur eine gefilterte Ausgabe der Daten bekommt. Bei einem View gibt man zum Beispiel an, dass von außen nur der Durchschnitt eines Werts einsehbar ist und man kann so im Idealfall nie auf die einzelnen Noten der Studenten zugreifen.
	\subsection{Audit}
	Beim Auditing wird für ein gewisses Event (Wie zum Beispiel ein Select der nicht erfolgreich war) das System nach unerwünschten Operationen überprüft. Dabei kann man auch angeben, welche Art der Abfrage (Select, Create, Delete, etc...) überprüft werden soll.
	\subsection{Role Based Access Control (RBAC)}
	Nahezu jedes Datenbanksystem verwendet ein Rollensystem um die Berechtigungen der Benutzer zu verwalten. So kann man pro Gruppe eine gewisse Menge an Zugriffsrechte definieren und muss nicht pro Benutzer die Rechte explizit angeben.
	\subsection{SQL Injection}
	Da die meisten Webanwendungen im Hintergrund eine Datenbank verwenden, wird getätigter Input an die Datenbank weitergegeben. Dabei ist es extrem wichtig, dass man diesem Input nicht implizit vertraut. Das hat damit zu tun, dass man eine SQL Query als Input an die Datenbank übergeben kann um so Daten auszulesen oder Schaden anzurichten.
	\subsubsection{Naive Authentifizierung}
	Wenn nur naiv Input der Datenbank übergeben wird kann man, zum Beispiel bei einer Passwortüberprüfung zusätzlich zum Passwort eine weitere Überprüfung mitgeben um so stets erfolgreich zu authentifizieren:
	\begin{verbatim}
	Select *
	FROM Studenten s JOIN prüfen p on s.MathNr = p.MathNr 
	WHERE s.Name = "Susanne" AND
								s.Passwort = "WilleUndVorstellung" OR "x"="x"
	\end{verbatim}
	Wenn der Input unsanitized übergeben wird, wird nicht nur auf das Passwort überprüft sondern zusätzlich ob x gleich x ist. Da x immer gleich x ist und durch das OR nur eine der beiden Überprüfungen wahr sein muss, kann man sich unabhängig vom Passwort erfolgreich authentifizieren.
	\paragraphlb{Vektor}
	Um so einen Angriff durchzuführen, muss man zuerst herausfinden wie das Datenmodell der Datenbank aussieht. Das kann man schaffen, indem man davor SELECT injected. So kann man zum Beispiel herausfinden, ob die Spalte als Zahl oder String oder sonstiges gespeichert wird.
	\paragraphlb{Prepared Statements}
	Ein effektiver Weg um innerhalb der Datenbank solche Injections zu vermeiden sind Prepared Statements. Da bei der Ausführung eines PS keine Daten in den Befehl eingesetzt werden, kann man sicherstellen, dass nur erlaubte Zugriffe abgesetzt werden.
	\subsubsection{Kyptographie}
	Um Daten nicht einfach aus der Datenbank auslesen zu können, kann man die gespeicherten Daten verschlüsseln. Passwörter sollten zum Beispiel nie in ihrer Reinform gespeichert werden, da man so selbst bei einem Datenleck die Information nicht zum Einloggen verwenden kann.
	\paragraphlb{Passwort vs. Public/Private Key}
	Man kann Passwörter auf einige Wege verschlüsseln: Entweder man verwendet ein Passwort um die Daten zu ver-/ und entzuschlüsseln. Das kann jedoch riskant sein, da so beide Parteien das Passwort benötigen. Hierbei ist ein Public/Private Key besser, da die Daten mittels Public Key verschlüsselt werden (Welcher von jedem einsehbar ist) aber nur mit dem Private Key wieder entschlüsselt werden können (Welchen nur der Benutzer hat und nie überträgt)
	\section{Fehlerbehandlung}
	Wenn ein Einbruch erfolgt ist, muss man die Daten sichern und feststellen welche Daten manipuliert wurden. Der erste Schritt ist dabei die Recovery Phase: Diese Klassifiziert den Fehler um diesen danach behandeln zu können. Diese Klassifikation geht von R1 bis R4:
	\begin{itemize}
		\item{R1}
		\begin{itemize}
			\item{Wenn ein Query noch nicht gespeichert wurde, kann man diesen löschen um so den Fehler beheben zu können.}
		\end{itemize}
		\item{R2}
		\item{R3}
		\item{R4}
	\end{itemize}
	Es gibt unterschiedliche Änderungsstrategien: Steal sowie Force
	
	\begin{itemize}
		\item{Steal}
		\begin{itemize}
			\item{[TODO]}
		\end{itemize}
		\item{Force}
		\begin{itemize}
			\item{Jede Transaktion muss sofort auf den Speicher geschrieben werden}
		\end{itemize}
	\end{itemize}
	\subsection{Änderungsstrategien}
	Es gibt verschiedene Strategien um Daten im Speicher zu verändern:
	\begin{itemize}
		\item{Duplication}
		\begin{itemize}
			\item{Daten werden stets überschrieben}
		\end{itemize}
		\item{Twin-Block}
		\begin{itemize}
			\item{[TODO]}
		\end{itemize}
		\item{Schattenblock}
	\end{itemize}
	\subsection{Logging}
	Um die Änderungen einer Datenbank abzuspeichern, benötigt man eine Loggingstrategie. Ein Logeintrag hat so stets ein gewisses Format:
	\begin{enumerate}
		\item{Nummer des Logeintrages}
		\item{Die Laufnummer der Transaktion}
		\item{Die ID der Page die verändert wird}
		\item{Die Strategie um den Befehl nochmal durchzuführen}
		\item{Die Strategie um den Befehl rückgängig zu machen}
		\item{Die ID des vorherigen Logeintrags}
	\end{enumerate}
	\subsubsection{Physisch/Logisch}
	Man unterscheidet zwischen einer physischen und einer logischen Protokollierung. Bei einer physischen Protokollierung wird jeweils ein image des vorherigen und nachfolgenden Status abgespeichert. Bei der lgoischen Abspeicherung wird der Zustand nicht direkt abgespeichert sondern nur die Veränderungen zum vorherigen Staus.
	\subsubsection{Wiederherstellung}
	Um Daten aus den Logs wiederherstellen zu können, muss man wissen, wo diese begonnen haben. Aus diesem Grund werden auf jeder Page die höchste Logsequenz abgespeichert.
	\paragraphlb{Speicherung}
	Da es passieren kann, dass im Event eines Datenverlustes auch die Logdateien verloren gehen, ist es normalerweise eine gute Idee, nur die ersten drei Logklassifizierungen vor Ort zu speichern und die vierte (und umfangreichste) in einem Archiv, welches an einem anderen physischen Ort sein sollte.
	\subsubsection{Speicherzeit}
	Man muss auch darüber nachdenken, wie lange Logs gespeichert werden sollten, da diese sehr viel Speicher benötigen können. Eine Strategie zur Speicherung ist der Log-Ring. Dabei hat das Log eine fixe Größe und sobald die Größe des Logs erreicht wurde, wird der älteste Eintrag überschrieben.
	\subsubsection{Write Ahead}
	Da das Logging heutzutage extrem wichtig ist, werden in der Regel auch Logs für Transaktionen geschrieben, welche noch nicht committed wurden. Das hilft dabei einen Fehler zu erkennen, welcher ein System zum Absturz bringt.
	\subsection{Wiederanlauf}
	Bei der Wiederherstellung einer Datenbank infolge eines Absturzes, muss man herausfinden, welche Transaktionen abgeschlossen wurden (Die Winner) und welche durch den Absturz unterbrochen wurden (Die Loser). Infolge eines Absturzes muss man dabei alle Loser rückgängig machen um den Zustand vor dem Absturz wiederherzustellen.
	\subsubsection{Kompensationseinträge}
	Wenn bei der Wiederherstellung Transaktionen vonnöten sind, müssen diese auch in das Log File geschrieben werden. Diese Einträge nennt man Kompensationseinträge und werden innerhalb des Logfiles mit spitzen Klammern <...> gekennzeichnet.
	\subsection{Sicherungspunkte}
	Wenn man eine Sicherung einer Datenbank erstellt, kann diese in drei Strategien vollbringen: Transaktionskonsistent, Aktionskonsistent und Fuzzy
	\begin{itemize}
		\item{Transaktionskonsistent}
		\begin{itemize}
			\item{Nach Abschluss einer Transaktion wird der Status gesichert.}
		\end{itemize}
		\item{Aktionskonsistent}
		\begin{itemize}
			\item{Statt ganzer Transaktionen werden die Aktionen innerhalb der Transaktionen als Speicherpunkt gesetzt wodurch es genauerer Zuordnung erlaubt}
		\end{itemize}
		\item{Fuzzy}
		\begin{itemize}
			\item{Eine minimale Speicherstrategie da nur die PageID sowie die minimale Änderung der Datenbank gespeichert wird. Man benötigt jedoch eine große Menge an Rechenleistung um das Logfile zu generieren.}
		\end{itemize}
	\end{itemize}	
	\section{Mehrbenutzersynchronisierung}
	Im Idealfall sollten natürlich mehrere Benutzer gleichzeitig auf einen Datenbank zugreifen können. Die grundlegendste Strategie dabei ist der Einzelbetrieb wobei eine Transaktion nach der anderen abgeschlossen wird. Eine weitere aber komplexere Strategie ist der Mehrbenutzerbetrieb bei dem jeder Transaktion eine gewisse Zeit zugeschrieben wird und diese somit "gleichzeitig" stattfinden.
	\subsection{Serialisierbarkeit}
	Zwei Transaktion sind serialisierbar, wenn diese nacheinander ausgeführt werden können um das gleiche Ergebnis zu erhalten. Das funktioniert nur, wenn eine Transaktion nicht von der anderen abhängig ist.
	\subsubsection{Theorie}
	Die Theorie der Serialisierbarkeit beschreibt, wann eine Transaktion serialisierbar ist. Diese besagt, dass zwei Transaktionen serialisierbar sind:
	\begin{itemize}
		\item{Beide T}
	\end{itemize}
	\section{Verteilte Umgebung}
	Da Datenbanken skalierbar sein sollten, sind verteilte Umgebungen essenziell. Diese haben den großen Vorteil, dass die Anwendung zwar jeweils die gleiche ist, jedoch auf vielen verschiedenen Servern läuft. Das kann man auf drei verschiedene Wege erreichen:
	\begin{itemize}
		\item{Asynchron}
		\begin{itemize}
			\item{Prozesse sind aktiv oder passiv wobei nur aktive Prozesse Nachrichten versenden}
		\end{itemize}
		\item{Synchron}
		\item{Atommodell}
	\end{itemize}
	\subsection{Potentielle Probleme}
	Ein Single-Point of Failure (SPOF) ist ein

	 













	
\end{document}